# src/preprocessing/crnp_processor.py

import pandas as pd
import numpy as np
import os
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union
from datetime import datetime

from ..core.logger import CRNPLogger, ProcessTimer
from ..utils.file_handler import FileHandler
from ..preprocessing.data_validator import DataValidator


class CRNPProcessor:
    """CRNP (ìš°ì£¼ì„  ì¤‘ì„±ì íƒì§€ê¸°) ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, station_config: Dict, processing_config: Dict, 
                 logger: Optional[CRNPLogger] = None):
        self.station_config = station_config
        self.processing_config = processing_config
        self.logger = logger or CRNPLogger("CRNPProcessor")
        
        # ì¢…ì† ëª¨ë“ˆ ì´ˆê¸°í™”
        self.file_handler = FileHandler(self.logger)
        self.validator = DataValidator(self.logger)
        
        # CRNP ë°ì´í„° í‘œì¤€ ì»¬ëŸ¼ëª…
        self.standard_columns = [
            'Timestamp', 'RN', 'Ta', 'RH', 'Pa', 
            'WS', 'WS_max', 'WD_VCT', 'N_counts'
        ]
        
        # TOA5 ì»¬ëŸ¼ ë§¤í•‘ (ë” í¬ê´„ì ì¸ ë§¤í•‘)
        self.toa5_column_mapping = {
            # í‘œì¤€ TOA5 ì»¬ëŸ¼ëª…
            'TIMESTAMP': 'Timestamp',
            'RECORD': 'RN',
            'Air_Temp_Avg': 'Ta',
            'RH_Avg': 'RH', 
            'Air_Press_Avg': 'Pa',
            'WS_avg_Avg': 'WS',
            'WS_max_Max': 'WS_max',
            'WD_VCT': 'WD_VCT',
            'HI_NeutronCts_Tot': 'N_counts',
            
            # ëŒ€ì•ˆì  TOA5 ì»¬ëŸ¼ëª…ë“¤
            'AirTC_Avg': 'Ta',
            'RH': 'RH',
            'BP_hPa_Avg': 'Pa',
            'WS_ms_Avg': 'WS',
            'WindDir_D1_WVT': 'WD_VCT',
            'Wind_Direction': 'WD_VCT',
            'NeutronCounts_Tot': 'N_counts',
            'CRD_Tot': 'N_counts',
            'CRNP_Tot': 'N_counts',
            
            # ì¶•ì•½í˜•ë“¤ (row 2ì—ì„œ ë‚˜íƒ€ë‚  ìˆ˜ ìˆëŠ”)
            'TS': 'Timestamp',  # Timestamp ì¶•ì•½í˜•
            'RN': 'RN',         # Record Number ì¶•ì•½í˜•
            'DegC': 'Ta',       # ì˜¨ë„ ë‹¨ìœ„
            '%': 'RH',          # ìŠµë„ ë‹¨ìœ„
            'hPa': 'Pa',        # ê¸°ì•• ë‹¨ìœ„
            'm/s': 'WS',        # í’ì† ë‹¨ìœ„
            'Deg': 'WD_VCT',    # í’í–¥ ë‹¨ìœ„
        }
        
        # ì»¬ëŸ¼ë³„ ì„¤ëª…
        self.column_descriptions = {
            'Timestamp': 'ì‹œê°„',
            'RN': 'ë ˆì½”ë“œ ë²ˆí˜¸',
            'Ta': 'ê¸°ì˜¨ (Â°C)',
            'RH': 'ìƒëŒ€ìŠµë„ (%)',
            'Pa': 'ê¸°ì•• (hPa)',
            'WS': 'í’ì† (m/s)',
            'WS_max': 'ìµœëŒ€í’ì† (m/s)',
            'WD_VCT': 'í’í–¥ (ë„)',
            'N_counts': 'ì¤‘ì„±ì ì¹´ìš´íŠ¸'
        }
        
    def process_crnp_data(self, output_dir: str) -> Dict[str, str]:
        """CRNP ë°ì´í„° ì „ì²´ ì²˜ë¦¬"""
        
        with ProcessTimer(self.logger, "CRNP Data Processing",
                         station=self.station_config['station_info']['id']):
            
            # 1. CRNP íŒŒì¼ ìë™ íƒì§€
            crnp_file = self._discover_crnp_file()
            
            # 2. ë°ì´í„° ì½ê¸° (TOA5 í˜•ì‹ ì „ìš©)
            raw_df = self._read_crnp_file(crnp_file)
            
            # 3. ê¸°ë³¸ ë°ì´í„° í’ˆì§ˆ í™•ì¸
            if len(raw_df) == 0:
                raise ValueError("No data found in CRNP file")
                
            if 'Timestamp' not in raw_df.columns:
                raise ValueError("Timestamp column not found after processing")
                
            # ì¤‘ì„±ì ì¹´ìš´íŠ¸ê°€ ì—†ì–´ë„ ê²½ê³ ë§Œ í•˜ê³  ê³„ì† ì§„í–‰
            if raw_df['N_counts'].isna().all():
                self.logger.warning("âš ï¸ No neutron counts data found - CRNP functionality will be limited")
                self.logger.warning("This may indicate:")
                self.logger.warning("  1. Missing neutron detector column in TOA5 file")
                self.logger.warning("  2. Column naming mismatch")
                self.logger.warning("  3. Data collection issue")
                self.logger.warning("Continuing with available meteorological data only...")
            else:
                neutron_count = raw_df['N_counts'].notna().sum()
                self.logger.info(f"âœ… Found {neutron_count} neutron count records")
            
            # 4. ë°ì´í„° ì „ì²˜ë¦¬
            processed_df = self._preprocess_crnp_data(raw_df)
            
            # 5. ì¶œë ¥ íŒŒì¼ ìƒì„±
            output_files = self._generate_outputs(processed_df, output_dir)
            
            # 6. ì²˜ë¦¬ ìš”ì•½
            self._log_processing_summary(processed_df)
            
            return output_files
            
    def _discover_crnp_file(self) -> str:
        """CRNP ë°ì´í„° íŒŒì¼ ìë™ íƒì§€"""
        crnp_folder = self.station_config['data_paths']['crnp_folder']
        
        if not os.path.exists(crnp_folder):
            raise FileNotFoundError(f"CRNP folder not found: {crnp_folder}")
            
        # ì§€ì›í•˜ëŠ” íŒŒì¼ í˜•ì‹ë“¤ íƒì§€
        excel_files = self.file_handler.discover_files(crnp_folder, "*.xlsx")
        csv_files = self.file_handler.discover_files(crnp_folder, "*.csv")
        
        all_files = excel_files + csv_files
        
        if not all_files:
            raise FileNotFoundError(f"No CRNP data files found in {crnp_folder}")
            
        # CRNP ê´€ë ¨ íŒŒì¼ ì°¾ê¸°
        crnp_files = []
        for file_path in all_files:
            filename = os.path.basename(file_path).lower()
            if any(keyword in filename for keyword in ['crnp', 'neutron', 'hourly']):
                crnp_files.append(file_path)
                
        if not crnp_files:
            # CRNP í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ íŒŒì¼ ì‚¬ìš©
            self.logger.warning("No files with CRNP keywords found, using first available file")
            crnp_files = [all_files[0]]
            
        if len(crnp_files) > 1:
            self.logger.warning(f"Multiple CRNP files found, using: {crnp_files[0]}")
            
        selected_file = crnp_files[0]
        self.logger.info(f"Selected CRNP file: {os.path.basename(selected_file)}")
        
        return selected_file
        
    def _read_crnp_file(self, file_path: str) -> pd.DataFrame:
        """CRNP íŒŒì¼ ì½ê¸° (TOA5 í˜•ì‹ ì „ìš©)"""
        
        try:
            # íŒŒì¼ ì •ë³´ ë¡œê¹…
            file_info = self.file_handler.get_file_info(file_path)
            self.logger.log_file_operation("read", file_path, "attempting", 
                                         size_mb=file_info['size_mb'])
            
            # íŒŒì¼ í˜•ì‹ ê°ì§€ ë° ì½ê¸°
            file_path_obj = Path(file_path)
            
            if file_path_obj.suffix.lower() in ['.xlsx', '.xls']:
                df = self._read_excel_toa5(file_path)
            elif file_path_obj.suffix.lower() in ['.csv', '.txt']:
                df = self._read_csv_toa5(file_path)
            else:
                raise ValueError(f"Unsupported file format: {file_path_obj.suffix}")
                
            # ê¸°ë³¸ ë°ì´í„° ì •ë³´ ë¡œê¹…
            if len(df) > 0:
                self.logger.info(f"Successfully read CRNP file: {len(df)} records, {len(df.columns)} columns")
                self.logger.info(f"Final columns: {list(df.columns)}")
            else:
                self.logger.warning("Empty dataframe after reading CRNP file")
            
            self.logger.log_data_summary(
                "CRNP_Raw", len(df),
                columns=len(df.columns),
                file_size_mb=file_info['size_mb']
            )
            
            return df
            
        except Exception as e:
            self.logger.log_error_with_context(e, f"Reading CRNP file {file_path}")
            raise
            
    def _read_excel_toa5(self, file_path: str) -> pd.DataFrame:
        """Excel TOA5 íŒŒì¼ ì½ê¸° - ì •í™•í•œ êµ¬ì¡° ë°˜ì˜"""
        
        # 1. ì „ì²´ í—¤ë” êµ¬ì¡° ë¶„ì„ (ì²˜ìŒ 6í–‰)
        header_df = pd.read_excel(file_path, header=None, nrows=6)
        
        self.logger.info("Analyzing TOA5 header structure:")
        for i in range(min(6, len(header_df))):
            row_data = header_df.iloc[i, :5].tolist()  # ì²˜ìŒ 5ê°œ ì»¬ëŸ¼ë§Œ
            self.logger.info(f"  Row {i}: {row_data}")
        
        # 2. TOA5 í˜•ì‹ í™•ì¸
        if len(header_df) < 4 or 'TOA5' not in str(header_df.iloc[0, 0]):
            raise ValueError("Not a valid TOA5 format file")
            
        self.logger.info("Processing Excel TOA5 format")
        
        # 3. ì‹¤ì œ ì»¬ëŸ¼ëª… ì¶”ì¶œ (í–‰ 1, 0-based) - ëŒ€ë¬¸ì ì»¬ëŸ¼ëª…ë“¤
        actual_columns = []
        if len(header_df) > 1:
            row1_data = header_df.iloc[1, :].tolist()
            # NaNì´ ì•„ë‹Œ ê°’ë“¤ë§Œ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ì‚¬ìš©
            actual_columns = [str(col) for col in row1_data if pd.notna(col)]
            self.logger.info(f"TOA5 columns from row 1: {actual_columns}")
        
        # 4. í–‰ 2ì˜ ì¶•ì•½í˜•ë„ í™•ì¸ (TS, RN, DegC ë“±)
        abbreviations = []
        if len(header_df) > 2:
            row2_data = header_df.iloc[2, :].tolist()
            abbreviations = [str(abbr) for abbr in row2_data if pd.notna(abbr)]
            self.logger.info(f"TOA5 abbreviations from row 2: {abbreviations}")
        
        # 5. ë°ì´í„° ë¶€ë¶„ ì½ê¸° (4í–‰ ìŠ¤í‚µ: 0=TOA5ë©”íƒ€, 1=ì»¬ëŸ¼ëª…, 2=ì¶•ì•½í˜•, 3=ê³µë€)
        data_df = pd.read_excel(file_path, skiprows=4)
        
        self.logger.info(f"Data shape after reading: {data_df.shape}")
        self.logger.info(f"Original data columns: {list(data_df.columns)}")
        
        # 6. ì»¬ëŸ¼ëª… ì„¤ì •
        if actual_columns and len(actual_columns) <= len(data_df.columns):
            # ì‹¤ì œ ì»¬ëŸ¼ëª…ì´ ìˆê³  ë°ì´í„° ì»¬ëŸ¼ ìˆ˜ë³´ë‹¤ ì ê±°ë‚˜ ê°™ìœ¼ë©´
            final_columns = actual_columns + [f"Col_{i}" for i in range(len(actual_columns), len(data_df.columns))]
            data_df.columns = final_columns[:len(data_df.columns)]
            self.logger.info(f"Applied TOA5 column names: {list(data_df.columns)}")
        else:
            # ì»¬ëŸ¼ëª… ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìœ¼ë©´ ê¸°ì¡´ ì»¬ëŸ¼ëª… ìœ ì§€
            self.logger.warning("Failed to extract proper column names, keeping original")
            
        # 7. ë°ì´í„° ìƒ˜í”Œ ë¡œê¹… (ë””ë²„ê¹…ìš©)
        if len(data_df) > 0:
            self.logger.info("Sample data (first 3 rows):")
            for i in range(min(3, len(data_df))):
                sample_row = data_df.iloc[i, :min(5, len(data_df.columns))].tolist()
                self.logger.info(f"  Data row {i}: {sample_row}")
        
        # 8. í‘œì¤€ CRNP ì»¬ëŸ¼ìœ¼ë¡œ ë§¤í•‘
        mapped_df = self._map_toa5_to_standard(data_df)
        
        return mapped_df
        
    def _read_csv_toa5(self, file_path: str) -> pd.DataFrame:
        """CSV TOA5 íŒŒì¼ ì½ê¸° - ì •í™•í•œ êµ¬ì¡° ë°˜ì˜"""
        
        encoding = self.file_handler.detect_encoding(str(file_path))
        
        # 1. ì „ì²´ í—¤ë” êµ¬ì¡° ë¶„ì„ (ì²˜ìŒ 6í–‰)
        header_df = pd.read_csv(file_path, encoding=encoding, header=None, nrows=6)
        
        self.logger.info("Analyzing CSV TOA5 header structure:")
        for i in range(min(6, len(header_df))):
            row_data = header_df.iloc[i, :5].tolist()  # ì²˜ìŒ 5ê°œ ì»¬ëŸ¼ë§Œ
            self.logger.info(f"  Row {i}: {row_data}")
        
        # 2. TOA5 í˜•ì‹ í™•ì¸
        if len(header_df) < 4 or 'TOA5' not in str(header_df.iloc[0, 0]):
            raise ValueError("Not a valid TOA5 format file")
            
        self.logger.info("Processing CSV TOA5 format")
        
        # 3. ì‹¤ì œ ì»¬ëŸ¼ëª… ì¶”ì¶œ (í–‰ 1, 0-based) - ëŒ€ë¬¸ì ì»¬ëŸ¼ëª…ë“¤
        actual_columns = []
        if len(header_df) > 1:
            row1_data = header_df.iloc[1, :].tolist()
            # NaNì´ ì•„ë‹Œ ê°’ë“¤ë§Œ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ì‚¬ìš©
            actual_columns = [str(col) for col in row1_data if pd.notna(col)]
            self.logger.info(f"TOA5 columns from row 1: {actual_columns}")
        
        # 4. í–‰ 2ì˜ ì¶•ì•½í˜•ë„ í™•ì¸ (TS, RN, DegC ë“±)
        abbreviations = []
        if len(header_df) > 2:
            row2_data = header_df.iloc[2, :].tolist()
            abbreviations = [str(abbr) for abbr in row2_data if pd.notna(abbr)]
            self.logger.info(f"TOA5 abbreviations from row 2: {abbreviations}")
        
        # 5. ë°ì´í„° ë¶€ë¶„ ì½ê¸° (4í–‰ ìŠ¤í‚µ: 0=TOA5ë©”íƒ€, 1=ì»¬ëŸ¼ëª…, 2=ì¶•ì•½í˜•, 3=ê³µë€)
        data_df = pd.read_csv(file_path, encoding=encoding, skiprows=4)
        
        self.logger.info(f"Data shape after reading: {data_df.shape}")
        self.logger.info(f"Original data columns: {list(data_df.columns)}")
        
        # 6. ì»¬ëŸ¼ëª… ì„¤ì •
        if actual_columns and len(actual_columns) <= len(data_df.columns):
            # ì‹¤ì œ ì»¬ëŸ¼ëª…ì´ ìˆê³  ë°ì´í„° ì»¬ëŸ¼ ìˆ˜ë³´ë‹¤ ì ê±°ë‚˜ ê°™ìœ¼ë©´
            final_columns = actual_columns + [f"Col_{i}" for i in range(len(actual_columns), len(data_df.columns))]
            data_df.columns = final_columns[:len(data_df.columns)]
            self.logger.info(f"Applied TOA5 column names: {list(data_df.columns)}")
        else:
            # ì»¬ëŸ¼ëª… ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìœ¼ë©´ ê¸°ì¡´ ì»¬ëŸ¼ëª… ìœ ì§€
            self.logger.warning("Failed to extract proper column names, keeping original")
            
        # 7. ë°ì´í„° ìƒ˜í”Œ ë¡œê¹… (ë””ë²„ê¹…ìš©)
        if len(data_df) > 0:
            self.logger.info("Sample data (first 3 rows):")
            for i in range(min(3, len(data_df))):
                sample_row = data_df.iloc[i, :min(5, len(data_df.columns))].tolist()
                self.logger.info(f"  Data row {i}: {sample_row}")
        
        # 8. í‘œì¤€ CRNP ì»¬ëŸ¼ìœ¼ë¡œ ë§¤í•‘
        mapped_df = self._map_toa5_to_standard(data_df)
        
        return mapped_df
        
    def _map_toa5_to_standard(self, df: pd.DataFrame) -> pd.DataFrame:
        """TOA5 ì»¬ëŸ¼ì„ í‘œì¤€ CRNP ì»¬ëŸ¼ìœ¼ë¡œ ë§¤í•‘ - ê°œì„ ëœ ë²„ì „"""
        
        self.logger.info("Mapping TOA5 columns to standard CRNP format")
        self.logger.info(f"Available columns: {list(df.columns)}")
        
        # ìƒˆ ë°ì´í„°í”„ë ˆì„ ìƒì„± (í‘œì¤€ ì»¬ëŸ¼ëª…ìœ¼ë¡œ)
        mapped_df = pd.DataFrame()
        
        # ê° í‘œì¤€ ì»¬ëŸ¼ì— ëŒ€í•´ ë§¤í•‘ ì°¾ê¸°
        for standard_col in self.standard_columns:
            mapped_value = None
            source_col = None
            
            # 1. ì§ì ‘ ë§¤í•‘ í™•ì¸ (ì •í™•í•œ ì´ë¦„)
            for toa5_col, std_col in self.toa5_column_mapping.items():
                if std_col == standard_col and toa5_col in df.columns:
                    mapped_value = df[toa5_col]
                    source_col = toa5_col
                    break
                    
            # 2. ë¶€ë¶„ ë§¤ì¹­ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œí•˜ê³  ë¶€ë¶„ ë¬¸ìì—´ í¬í•¨)
            if mapped_value is None:
                for col in df.columns:
                    if self._is_matching_column(col, standard_col):
                        mapped_value = df[col]
                        source_col = col
                        break
                        
            # 3. íŠ¹ë³„í•œ ê²½ìš° ì²˜ë¦¬ (ì¤‘ì„±ì ì¹´ìš´íŠ¸)
            if mapped_value is None and standard_col == 'N_counts':
                # ì¤‘ì„±ì ê´€ë ¨ í‚¤ì›Œë“œë¡œ ì°¾ê¸°
                neutron_keywords = ['neutron', 'counts', 'hi_neutron', 'crnp', 'cosmic', 'count']
                for col in df.columns:
                    col_lower = str(col).lower()
                    if any(keyword in col_lower for keyword in neutron_keywords):
                        # ìˆ«ì ë°ì´í„°ì¸ì§€ í™•ì¸
                        try:
                            numeric_data = pd.to_numeric(df[col], errors='coerce')
                            if numeric_data.notna().sum() > 0:
                                mapped_value = df[col]
                                source_col = col
                                self.logger.info(f"Found potential neutron column: {col}")
                                break
                        except:
                            continue
                            
            # 4. ìœ„ì¹˜ ê¸°ë°˜ ë§¤í•‘ (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
            if mapped_value is None and standard_col == 'N_counts':
                # ë§ˆì§€ë§‰ ì»¬ëŸ¼ì´ ì¤‘ì„±ì ì¹´ìš´íŠ¸ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
                if len(df.columns) > 0:
                    last_col = df.columns[-1]
                    try:
                        numeric_data = pd.to_numeric(df[last_col], errors='coerce')
                        if numeric_data.notna().sum() > 0 and numeric_data.mean() > 10:  # ì¤‘ì„±ì ì¹´ìš´íŠ¸ëŠ” ë³´í†µ í° ê°’
                            mapped_value = df[last_col]
                            source_col = f"{last_col} (position-based)"
                            self.logger.info(f"Using last column as neutron counts: {last_col}")
                    except:
                        pass
                        
            # ë§¤í•‘ ê²°ê³¼ ì ìš©
            if mapped_value is not None:
                mapped_df[standard_col] = mapped_value
                self.logger.info(f"  âœ… {standard_col} â† {source_col}")
            else:
                mapped_df[standard_col] = np.nan
                self.logger.warning(f"  âŒ {standard_col} â† (missing)")
                
        # ë§¤í•‘ ê²°ê³¼ ìš”ì•½ ë° íŠ¹ë³„ ì²˜ë¦¬
        mapped_count = sum(1 for col in self.standard_columns if mapped_df[col].notna().any())
        self.logger.info(f"Mapping complete: {mapped_count}/{len(self.standard_columns)} columns mapped")
        
        # ì¤‘ì„±ì ì¹´ìš´íŠ¸ê°€ ì—†ëŠ” ê²½ìš° íŠ¹ë³„ ë¡œê¹…
        if mapped_df['N_counts'].isna().all():
            self.logger.error("âš ï¸ CRITICAL: No neutron counts column found!")
            self.logger.error("Available columns for analysis:")
            for i, col in enumerate(df.columns):
                sample_values = df[col].head(3).tolist()
                self.logger.error(f"  [{i}] '{col}': {sample_values}")
                
            # ìˆ˜ë™ ë§¤í•‘ ì‹œë„ (ëª¨ë“  ìˆ«ì ì»¬ëŸ¼ í™•ì¸)
            self.logger.error("Attempting manual neutron detection...")
            for col in df.columns:
                try:
                    numeric_data = pd.to_numeric(df[col], errors='coerce')
                    if numeric_data.notna().sum() > 0:
                        mean_val = numeric_data.mean()
                        std_val = numeric_data.std()
                        min_val = numeric_data.min()
                        max_val = numeric_data.max()
                        self.logger.error(f"  Numeric column '{col}': mean={mean_val:.1f}, std={std_val:.1f}, range=[{min_val:.1f}, {max_val:.1f}]")
                        
                        # ì¤‘ì„±ì ì¹´ìš´íŠ¸ íŠ¹ì„± í™•ì¸ (ë³´í†µ 100-10000 ë²”ìœ„, ë†’ì€ ë³€ë™ì„±)
                        if 50 < mean_val < 50000 and std_val > mean_val * 0.1:
                            self.logger.error(f"  ğŸ‘† '{col}' might be neutron counts (will use as fallback)")
                            mapped_df['N_counts'] = df[col]
                            break
                except:
                    continue
        else:
            # ì¤‘ì„±ì ì¹´ìš´íŠ¸ ë°ì´í„° ìš”ì•½
            neutron_data = mapped_df['N_counts'].dropna()
            if len(neutron_data) > 0:
                self.logger.info(f"âœ… Neutron counts found: mean={neutron_data.mean():.1f}, range=[{neutron_data.min():.1f}, {neutron_data.max():.1f}]")
        
        return mapped_df
        
    def _is_matching_column(self, col_name: str, standard_col: str) -> bool:
        """ì»¬ëŸ¼ëª… ë§¤ì¹­ í™•ì¸ - ê°œì„ ëœ ë²„ì „"""
        
        col_lower = str(col_name).lower().replace('_', '').replace('-', '')
        
        # í‘œì¤€ ì»¬ëŸ¼ë³„ í‚¤ì›Œë“œ ë§¤ì¹­ (ìš°ì„ ìˆœìœ„ ìˆœ)
        matching_patterns = {
            'Timestamp': [
                ['timestamp', 'time'],
                ['date'],
                ['ts']  # TOA5ì—ì„œ ìì£¼ ì‚¬ìš©
            ],
            'RN': [
                ['record'],
                ['rn'],
                ['rec']
            ],
            'Ta': [
                ['airtemp', 'air_temp'],
                ['temp', 'temperature'],
                ['ta'],
                ['degc']  # ë‹¨ìœ„ëª…ë„ í™•ì¸
            ],
            'RH': [
                ['rh', 'relativehumidity'],
                ['humidity', 'humid'],
                ['%']  # ë‹¨ìœ„ë¡œë„ í™•ì¸
            ],
            'Pa': [
                ['airpress', 'air_press', 'pressure'],
                ['pa', 'press'],
                ['hpa']  # ë‹¨ìœ„ëª…
            ],
            'WS': [
                ['windspeed', 'wsavg'],
                ['ws', 'wind'],
                ['m/s']  # ë‹¨ìœ„ëª…
            ],
            'WS_max': [
                ['windmax', 'wsmax'],
                ['maxwind', 'windgust']
            ],
            'WD_VCT': [
                ['winddir', 'wd'],
                ['direction', 'dir'],
                ['deg']  # ë‹¨ìœ„ëª…
            ],
            'N_counts': [
                ['neutron', 'cosmic'],
                ['counts', 'count', 'cnt'],
                ['hi', 'crnp'],
                ['tot', 'total']  # TOA5ì—ì„œ _Tot ìì£¼ ì‚¬ìš©
            ]
        }
        
        if standard_col in matching_patterns:
            # ìš°ì„ ìˆœìœ„ë³„ë¡œ ë§¤ì¹­ í™•ì¸
            for pattern_group in matching_patterns[standard_col]:
                if all(pattern in col_lower for pattern in pattern_group):
                    return True
                    
            # ë‹¨ì¼ í‚¤ì›Œë“œ ë§¤ì¹­
            for pattern_group in matching_patterns[standard_col]:
                for pattern in pattern_group:
                    if pattern in col_lower:
                        return True
                        
        return False
            
    def _preprocess_crnp_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """CRNP ë°ì´í„° ì „ì²˜ë¦¬"""
        
        processed_df = df.copy()
        
        self.logger.info("CRNP data preprocessing started")
        self.logger.info(f"Original data shape: {processed_df.shape}")
        
        # 1. íƒ€ì„ìŠ¤íƒ¬í”„ ì²˜ë¦¬
        with ProcessTimer(self.logger, "Timestamp Processing"):
            
            if 'Timestamp' in processed_df.columns:
                # ì´ë¯¸ datetime ê°ì²´ì¸ì§€ í™•ì¸
                sample_timestamp = processed_df['Timestamp'].iloc[0] if len(processed_df) > 0 else None
                
                if isinstance(sample_timestamp, (pd.Timestamp, datetime)):
                    self.logger.info("Timestamp already in datetime format")
                    processed_df['timestamp'] = processed_df['Timestamp']
                else:
                    self.logger.info("Converting timestamp from string/numeric format")
                    processed_df['timestamp'] = pd.to_datetime(processed_df['Timestamp'], errors='coerce')
                    
                # ìœ íš¨í•˜ì§€ ì•Šì€ íƒ€ì„ìŠ¤íƒ¬í”„ í™•ì¸
                invalid_timestamps = processed_df['timestamp'].isna().sum()
                if invalid_timestamps > 0:
                    self.logger.warning(f"Found {invalid_timestamps} invalid timestamps")
                    
                # ìœ íš¨í•œ íƒ€ì„ìŠ¤íƒ¬í”„ë§Œ ìœ ì§€
                initial_count = len(processed_df)
                processed_df = processed_df.dropna(subset=['timestamp'])
                removed_count = initial_count - len(processed_df)
                
                if removed_count > 0:
                    self.logger.warning(f"Removed {removed_count} records with invalid timestamps")
                    
                # ìµœì¢… íƒ€ì„ìŠ¤íƒ¬í”„ ê²€ì¦
                if len(processed_df) > 0:
                    final_date_range = f"{processed_df['timestamp'].min()} to {processed_df['timestamp'].max()}"
                    self.logger.info(f"Final timestamp range: {final_date_range}")
            else:
                raise ValueError("Timestamp column not found")
                
        # 2. ìˆ˜ì¹˜ ë°ì´í„° ë³€í™˜
        numeric_columns = ['RN', 'Ta', 'RH', 'Pa', 'WS', 'WS_max', 'WD_VCT', 'N_counts']
        
        for col in numeric_columns:
            if col in processed_df.columns:
                original_values = processed_df[col].notna().sum()
                processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce')
                converted_values = processed_df[col].notna().sum()
                
                if original_values != converted_values:
                    self.logger.warning(f"Lost {original_values - converted_values} values during numeric conversion in {col}")
                    
        # 3. ì¤‘ì„±ì ì¹´ìš´íŠ¸ íŠ¹ë³„ ì²˜ë¦¬
        if 'N_counts' in processed_df.columns:
            # 0 ì´í•˜ ê°’ì„ NaNìœ¼ë¡œ ë³€ê²½
            invalid_neutrons = (processed_df['N_counts'] <= 0).sum()
            if invalid_neutrons > 0:
                self.logger.warning(f"Found {invalid_neutrons} non-positive neutron counts, setting to NaN")
                processed_df.loc[processed_df['N_counts'] <= 0, 'N_counts'] = np.nan
                
        # 4. ê¸°ìƒ ë°ì´í„° ë²”ìœ„ í™•ì¸ ë° ìˆ˜ì •
        processed_df = self._apply_range_limits(processed_df)
        
        # 5. ì‹œê°„ ìˆœì„œ ì •ë ¬
        processed_df = processed_df.sort_values('timestamp').reset_index(drop=True)
        
        if len(processed_df) > 0:
            self.logger.log_data_summary(
                "CRNP_Processed", len(processed_df),
                date_range=f"{processed_df['timestamp'].min()} to {processed_df['timestamp'].max()}"
            )
        else:
            self.logger.error("No data remaining after preprocessing")
        
        return processed_df
        
    def _apply_range_limits(self, df: pd.DataFrame) -> pd.DataFrame:
        """ë¬¼ë¦¬ì  ë²”ìœ„ ì œí•œ ì ìš©"""
        
        range_limits = {
            'Ta': (-40, 50),      # ê¸°ì˜¨
            'RH': (0, 100),       # ìƒëŒ€ìŠµë„
            'Pa': (800, 1100),    # ê¸°ì••
            'WS': (0, 50),        # í’ì†
            'WS_max': (0, 50),    # ìµœëŒ€í’ì†
            'WD_VCT': (0, 360),   # í’í–¥
        }
        
        processed_df = df.copy()
        
        for column, (min_val, max_val) in range_limits.items():
            if column in processed_df.columns:
                out_of_range = ((processed_df[column] < min_val) | 
                               (processed_df[column] > max_val))
                out_of_range_count = out_of_range.sum()
                
                if out_of_range_count > 0:
                    self.logger.warning(f"Found {out_of_range_count} out-of-range values in {column}, setting to NaN")
                    processed_df.loc[out_of_range, column] = np.nan
                    
        return processed_df
        
    def _generate_outputs(self, df: pd.DataFrame, output_dir: str) -> Dict[str, str]:
        """ì¶œë ¥ íŒŒì¼ ìƒì„±"""
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        station_id = self.station_config['station_info']['id']
        output_files = {}
        
        # 1. ì „ì²˜ë¦¬ëœ CRNP ë°ì´í„° (ì…ë ¥ í˜•ì‹)
        with ProcessTimer(self.logger, "Generating CRNP input file"):
            input_file = output_path / f"{station_id}_CRNP_input.xlsx"
            self.file_handler.save_dataframe(df, str(input_file), index=False)
            output_files['input_format'] = str(input_file)
            
        # 2. í’ˆì§ˆ ë³´ê³ ì„œ
        with ProcessTimer(self.logger, "Generating quality report"):
            quality_report = self._generate_quality_report(df)
            report_file = output_path / f"{station_id}_CRNP_quality_report.txt"
            
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(quality_report)
                
            output_files['quality_report'] = str(report_file)
            
        return output_files
        
    def _generate_quality_report(self, df: pd.DataFrame) -> str:
        """í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±"""
        
        lines = []
        lines.append("=" * 60)
        lines.append("CRNP DATA QUALITY REPORT")
        lines.append("=" * 60)
        lines.append(f"Report Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append(f"Station: {self.station_config['station_info']['name']}")
        lines.append(f"Total Records: {len(df)}")
        
        if 'timestamp' in df.columns and len(df) > 0:
            lines.append(f"Date Range: {df['timestamp'].min()} to {df['timestamp'].max()}")
            
        lines.append("")
        lines.append("DATA COMPLETENESS:")
        lines.append("-" * 30)
        
        total_records = len(df)
        for col in self.standard_columns[1:]:  # Timestamp ì œì™¸
            if col in df.columns:
                valid_count = df[col].notna().sum()
                completeness = (valid_count / total_records) * 100 if total_records > 0 else 0
                desc = self.column_descriptions.get(col, col)
                lines.append(f"{desc:15} ({col:8}): {completeness:6.2f}% ({valid_count:5}/{total_records})")
                
        lines.append("")
        lines.append("DATA SUMMARY STATISTICS:")
        lines.append("-" * 30)
        
        # ì£¼ìš” ë³€ìˆ˜ë“¤ì˜ í†µê³„
        numeric_columns = ['Ta', 'RH', 'Pa', 'N_counts']
        for col in numeric_columns:
            if col in df.columns and df[col].notna().sum() > 0:
                try:
                    desc = self.column_descriptions.get(col, col)
                    stats = df[col].describe()
                    lines.append(f"{desc} ({col}):")
                    lines.append(f"  Mean: {stats['mean']:.2f}, Std: {stats['std']:.2f}")
                    lines.append(f"  Min: {stats['min']:.2f}, Max: {stats['max']:.2f}")
                    lines.append("")
                except Exception as e:
                    lines.append(f"{desc} ({col}): Statistics calculation failed - {e}")
                    lines.append("")
            elif col == 'N_counts':
                lines.append("ì¤‘ì„±ì ì¹´ìš´íŠ¸ (N_counts): âŒ ë°ì´í„° ì—†ìŒ")
                lines.append("  ê²½ê³ : CRNP ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
                lines.append("")
                
        lines.append("=" * 60)
        return "\n".join(lines)
        
    def _log_processing_summary(self, df: pd.DataFrame) -> None:
        """ì²˜ë¦¬ ìš”ì•½ ë¡œê¹…"""
        
        summary = {
            'total_records': len(df),
            'date_range': "Unknown",
            'completeness': {}
        }
        
        if 'timestamp' in df.columns and len(df) > 0:
            summary['date_range'] = f"{df['timestamp'].min()} to {df['timestamp'].max()}"
        
        # ë°ì´í„° ì™„ì„±ë„ ê³„ì‚°
        for col in self.standard_columns[1:]:
            if col in df.columns:
                completeness = (df[col].notna().sum() / len(df)) * 100 if len(df) > 0 else 0
                summary['completeness'][col] = round(completeness, 1)
                
        self.logger.info(f"CRNP processing summary: {summary['total_records']} records, {summary['date_range']}")
        
        # ì£¼ìš” ë³€ìˆ˜ë“¤ì˜ ì™„ì„±ë„ ë¡œê¹…
        key_variables = ['N_counts', 'Ta', 'Pa', 'RH']
        for var in key_variables:
            if var in summary['completeness']:
                if var == 'N_counts' and summary['completeness'][var] == 0:
                    self.logger.warning(f"  {var}: {summary['completeness'][var]}% complete âš ï¸ MISSING - CRNP functionality limited")
                else:
                    self.logger.info(f"  {var}: {summary['completeness'][var]}% complete")
                    
        # ì¤‘ì„±ì ì¹´ìš´íŠ¸ ëˆ„ë½ ì‹œ ì¶”ê°€ ì•ˆë‚´
        if summary['completeness'].get('N_counts', 0) == 0:
            self.logger.warning("âš ï¸ IMPORTANT: No neutron count data processed")
            self.logger.warning("  - Calibration will not be possible")
            self.logger.warning("  - Only meteorological data is available")
            self.logger.warning("  - Check original TOA5 file for neutron detector columns")


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ìš© ì„¤ì •
    test_station_config = {
        'station_info': {'id': 'PC', 'name': 'Pyeongchang Station'},
        'data_paths': {
            'crnp_folder': 'data/input/PC/crnp/'
        }
    }
    
    test_processing_config = {}
    
    # CRNPProcessor í…ŒìŠ¤íŠ¸
    from ..core.logger import setup_logger
    
    logger = setup_logger("CRNPProcessor_Test")
    processor = CRNPProcessor(test_station_config, test_processing_config, logger)
    
    try:
        output_files = processor.process_crnp_data("data/output/PC/preprocessed/")
        print("âœ… CRNP ì²˜ë¦¬ ì™„ë£Œ!")
        print("ìƒì„±ëœ íŒŒì¼ë“¤:")
        for output_type, file_path in output_files.items():
            print(f"  {output_type}: {file_path}")
            
    except Exception as e:
        print(f"âŒ CRNP ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()