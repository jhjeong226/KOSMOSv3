# src/preprocessing/preprocessing_pipeline.py

import os
import sys
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
current_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(current_dir))

from src.core.config_manager import ConfigManager
from src.core.logger import CRNPLogger, ProcessTimer
from src.preprocessing.fdr_processor import FDRProcessor
from src.preprocessing.crnp_processor import CRNPProcessor


class PreprocessingPipeline:
    """ë°ì´í„° ì „ì²˜ë¦¬ í†µí•© íŒŒì´í”„ë¼ì¸ - ê°„ì†Œí™”ëœ ë²„ì „"""
    
    def __init__(self, config_root: str = "config"):
        self.config_manager = ConfigManager(config_root)
        self.main_logger = CRNPLogger("PreprocessingPipeline")
        self.results = {}
        
    def run_station_preprocessing(self, station_id: str, 
                                output_base_dir: str = "data/output") -> Dict[str, Any]:
        """ë‹¨ì¼ ê´€ì¸¡ì†Œ ì „ì²˜ë¦¬ ì‹¤í–‰"""
        
        with ProcessTimer(self.main_logger, f"Station {station_id} Preprocessing"):
            
            try:
                # 1. ì„¤ì • ë¡œë“œ
                station_config, processing_config = self._load_configurations(station_id)
                
                # 2. ê´€ì¸¡ì†Œë³„ ë¡œê±° ìƒì„±
                station_logger = self.main_logger.create_station_logger(station_id)
                
                # 3. ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
                output_dir = Path(output_base_dir) / station_id / "preprocessed"
                output_dir.mkdir(parents=True, exist_ok=True)
                
                # 4. FDR ë°ì´í„° ì²˜ë¦¬
                fdr_results = self._process_fdr_data(
                    station_config, processing_config, station_logger, str(output_dir)
                )
                
                # 5. CRNP ë°ì´í„° ì²˜ë¦¬
                crnp_results = self._process_crnp_data(
                    station_config, processing_config, station_logger, str(output_dir)
                )
                
                # 6. ê²°ê³¼ í†µí•©
                combined_results = self._combine_results(
                    station_id, fdr_results, crnp_results, str(output_dir)
                )
                
                # 7. ë©”íƒ€ë°ì´í„° ì €ì¥
                self._save_metadata(combined_results, str(output_dir))
                
                self.results[station_id] = combined_results
                station_logger.info("Preprocessing completed successfully")
                
                return combined_results
                
            except Exception as e:
                self.main_logger.log_error_with_context(e, f"Station {station_id} preprocessing")
                raise
                
    def run_multiple_stations(self, station_ids: List[str], 
                             output_base_dir: str = "data/output",
                             parallel: bool = False) -> Dict[str, Any]:
        """ë‹¤ì¤‘ ê´€ì¸¡ì†Œ ì „ì²˜ë¦¬ ì‹¤í–‰"""
        
        if parallel and len(station_ids) > 1:
            return self._run_parallel_preprocessing(station_ids, output_base_dir)
        else:
            return self._run_sequential_preprocessing(station_ids, output_base_dir)
            
    def _load_configurations(self, station_id: str) -> tuple:
        """ì„¤ì • íŒŒì¼ë“¤ ë¡œë“œ"""
        
        try:
            station_config = self.config_manager.load_station_config(station_id)
            processing_config = self.config_manager.load_processing_config()
            
            # í•„ìˆ˜ ê²½ë¡œ ê²€ì¦
            self._validate_station_paths(station_config)
            
            return station_config, processing_config
            
        except Exception as e:
            self.main_logger.log_error_with_context(e, f"Loading configuration for {station_id}")
            raise
            
    def _validate_station_paths(self, station_config: Dict) -> None:
        """ê´€ì¸¡ì†Œ ê²½ë¡œ ìœ íš¨ì„± ê²€ì¦"""
        
        paths_to_check = [
            ('fdr_folder', 'FDR data folder'),
            ('crnp_folder', 'CRNP data folder')
        ]
        
        missing_paths = []
        
        for path_key, description in paths_to_check:
            path = station_config['data_paths'].get(path_key)
            if path and not os.path.exists(path):
                missing_paths.append(f"{description}: {path}")
                
        if missing_paths:
            self.main_logger.warning(f"Missing paths: {', '.join(missing_paths)}")
            
    def _process_fdr_data(self, station_config: Dict, processing_config: Dict,
                         logger: CRNPLogger, output_dir: str) -> Dict[str, Any]:
        """FDR ë°ì´í„° ì²˜ë¦¬"""
        
        try:
            fdr_processor = FDRProcessor(station_config, processing_config, logger)
            output_files = fdr_processor.process_all_fdr_data(output_dir)
            
            # ì²˜ë¦¬ ìš”ì•½ ìƒì„±
            summary = self._get_fdr_summary(output_files.get('input_format'))
            
            logger.info(f"FDR processing completed: {len(output_files)} files")
            
            return {
                'status': 'success',
                'output_files': output_files,
                'summary': summary,
                'processor': 'FDRProcessor'
            }
            
        except Exception as e:
            logger.log_error_with_context(e, "FDR data processing")
            return {
                'status': 'failed',
                'error': str(e),
                'output_files': {},
                'summary': {},
                'processor': 'FDRProcessor'
            }
            
    def _process_crnp_data(self, station_config: Dict, processing_config: Dict,
                          logger: CRNPLogger, output_dir: str) -> Dict[str, Any]:
        """CRNP ë°ì´í„° ì²˜ë¦¬"""
        
        try:
            crnp_processor = CRNPProcessor(station_config, processing_config, logger)
            output_files = crnp_processor.process_crnp_data(output_dir)
            
            # ì²˜ë¦¬ ìš”ì•½ ìƒì„±
            summary = self._get_crnp_summary(output_files.get('input_format'))
            
            logger.info(f"CRNP processing completed: {len(output_files)} files")
            
            return {
                'status': 'success',
                'output_files': output_files,
                'summary': summary,
                'processor': 'CRNPProcessor'
            }
            
        except Exception as e:
            logger.log_error_with_context(e, "CRNP data processing")
            return {
                'status': 'failed',
                'error': str(e),
                'output_files': {},
                'summary': {},
                'processor': 'CRNPProcessor'
            }
            
    def _get_fdr_summary(self, input_file: Optional[str]) -> Dict:
        """FDR ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        
        if not input_file or not os.path.exists(input_file):
            return {}
            
        try:
            import pandas as pd
            df = pd.read_excel(input_file)
            
            return {
                'total_records': len(df),
                'sensors': df['id'].nunique() if 'id' in df.columns else 0,
                'depths': sorted(df['FDR_depth'].unique()) if 'FDR_depth' in df.columns else [],
                'date_range': {
                    'start': str(df['Date'].min()) if 'Date' in df.columns else None,
                    'end': str(df['Date'].max()) if 'Date' in df.columns else None
                }
            }
            
        except Exception as e:
            self.main_logger.warning(f"Could not generate FDR summary: {e}")
            return {}
            
    def _get_crnp_summary(self, input_file: Optional[str]) -> Dict:
        """CRNP ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        
        if not input_file or not os.path.exists(input_file):
            return {}
            
        try:
            import pandas as pd
            df = pd.read_excel(input_file)
            
            summary = {
                'total_records': len(df),
                'date_range': {
                    'start': str(df['timestamp'].min()) if 'timestamp' in df.columns else None,
                    'end': str(df['timestamp'].max()) if 'timestamp' in df.columns else None
                }
            }
            
            # ì£¼ìš” ë³€ìˆ˜ë“¤ì˜ ì™„ì„±ë„ ê³„ì‚°
            key_columns = ['N_counts', 'Ta', 'RH', 'Pa']
            completeness = {}
            for col in key_columns:
                if col in df.columns:
                    complete_pct = (df[col].notna().sum() / len(df)) * 100
                    completeness[col] = round(complete_pct, 1)
                    
            summary['data_completeness'] = completeness
            return summary
            
        except Exception as e:
            self.main_logger.warning(f"Could not generate CRNP summary: {e}")
            return {}
            
    def _combine_results(self, station_id: str, fdr_results: Dict, 
                        crnp_results: Dict, output_dir: str) -> Dict[str, Any]:
        """FDRê³¼ CRNP ê²°ê³¼ í†µí•©"""
        
        combined_results = {
            'station_id': station_id,
            'processing_timestamp': datetime.now().isoformat(),
            'overall_status': 'success',
            'fdr': fdr_results,
            'crnp': crnp_results,
            'output_directory': output_dir
        }
        
        # ì „ì²´ ìƒíƒœ ê²°ì •
        if fdr_results.get('status') == 'failed' or crnp_results.get('status') == 'failed':
            combined_results['overall_status'] = 'partial_success'
            
        if fdr_results.get('status') == 'failed' and crnp_results.get('status') == 'failed':
            combined_results['overall_status'] = 'failed'
            
        return combined_results
        
    def _save_metadata(self, results: Dict, output_dir: str) -> None:
        """ì²˜ë¦¬ ë©”íƒ€ë°ì´í„° ì €ì¥"""
        
        metadata_file = Path(output_dir) / "preprocessing_metadata.json"
        
        try:
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False, default=str)
                
            self.main_logger.log_file_operation("save", str(metadata_file), "success")
            
        except Exception as e:
            self.main_logger.log_error_with_context(e, f"Saving metadata")
            
    def _run_sequential_preprocessing(self, station_ids: List[str], 
                                    output_base_dir: str) -> Dict[str, Any]:
        """ìˆœì°¨ì  ë‹¤ì¤‘ ê´€ì¸¡ì†Œ ì²˜ë¦¬"""
        
        results = {}
        
        for station_id in station_ids:
            try:
                station_result = self.run_station_preprocessing(station_id, output_base_dir)
                results[station_id] = station_result
                
            except Exception as e:
                self.main_logger.log_error_with_context(e, f"Processing station {station_id}")
                results[station_id] = {
                    'station_id': station_id,
                    'overall_status': 'failed',
                    'error': str(e)
                }
                
        return results
        
    def _run_parallel_preprocessing(self, station_ids: List[str], 
                                  output_base_dir: str) -> Dict[str, Any]:
        """ë³‘ë ¬ ë‹¤ì¤‘ ê´€ì¸¡ì†Œ ì²˜ë¦¬"""
        
        import concurrent.futures
        import threading
        
        results = {}
        results_lock = threading.Lock()
        
        def process_station(station_id):
            try:
                result = self.run_station_preprocessing(station_id, output_base_dir)
                with results_lock:
                    results[station_id] = result
            except Exception as e:
                with results_lock:
                    results[station_id] = {
                        'station_id': station_id,
                        'overall_status': 'failed',
                        'error': str(e)
                    }
                    
        max_workers = min(len(station_ids), 4)
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(process_station, station_id): station_id 
                      for station_id in station_ids}
            
            for future in concurrent.futures.as_completed(futures):
                station_id = futures[future]
                try:
                    future.result()
                    self.main_logger.info(f"Completed {station_id}")
                except Exception as e:
                    self.main_logger.log_error_with_context(e, f"Parallel processing of {station_id}")
                    
        return results
        
    def generate_summary_report(self, output_dir: str = "data/output") -> str:
        """ì „ì²´ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ë³´ê³ ì„œ ìƒì„±"""
        
        lines = []
        lines.append("=" * 80)
        lines.append("CRNP DATA PREPROCESSING SUMMARY REPORT")
        lines.append("=" * 80)
        lines.append(f"Report Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append(f"Total Stations Processed: {len(self.results)}")
        lines.append("")
        
        for station_id, result in self.results.items():
            lines.append(f"STATION: {station_id}")
            lines.append("-" * 40)
            lines.append(f"Overall Status: {result.get('overall_status', 'Unknown')}")
            
            # FDR ìš”ì•½
            fdr_status = result.get('fdr', {}).get('status', 'Unknown')
            lines.append(f"FDR Processing: {fdr_status}")
            
            fdr_summary = result.get('fdr', {}).get('summary', {})
            if fdr_summary:
                lines.append(f"  - Sensors: {fdr_summary.get('sensors', 0)}")
                lines.append(f"  - Records: {fdr_summary.get('total_records', 0)}")
                
            # CRNP ìš”ì•½
            crnp_status = result.get('crnp', {}).get('status', 'Unknown')
            lines.append(f"CRNP Processing: {crnp_status}")
            
            crnp_summary = result.get('crnp', {}).get('summary', {})
            if crnp_summary:
                lines.append(f"  - Records: {crnp_summary.get('total_records', 0)}")
                
            lines.append("")
            
        lines.append("=" * 80)
        
        report_content = "\n".join(lines)
        
        # ë³´ê³ ì„œ íŒŒì¼ ì €ì¥
        report_file = Path(output_dir) / "preprocessing_summary_report.txt"
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report_content)
            self.main_logger.info(f"Summary report saved")
        except Exception as e:
            self.main_logger.warning(f"Could not save summary report: {e}")
            
        return report_content


# ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸš€ CRNP ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("=" * 60)
    
    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipeline = PreprocessingPipeline()
    
    # ë‹¨ì¼ ê´€ì¸¡ì†Œ ì²˜ë¦¬
    try:
        print("ğŸ“ ê´€ì¸¡ì†Œ ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
        
        hc_results = pipeline.run_station_preprocessing("HC")
        
        if hc_results['overall_status'] == 'success':
            print("âœ… ê´€ì¸¡ì†Œ ì²˜ë¦¬ ì™„ë£Œ!")
        else:
            print(f"âš ï¸  ê´€ì¸¡ì†Œ ì²˜ë¦¬ ë¶€ë¶„ ì™„ë£Œ: {hc_results['overall_status']}")
            
        # ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
        print(f"  FDR ì²˜ë¦¬: {hc_results['fdr']['status']}")
        print(f"  CRNP ì²˜ë¦¬: {hc_results['crnp']['status']}")
        
        # ìš”ì•½ ë³´ê³ ì„œ ìƒì„±
        print("\nğŸ“‹ ìš”ì•½ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        summary_report = pipeline.generate_summary_report()
        print("ìš”ì•½ ë³´ê³ ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        
    print("\n" + "=" * 60)
    print("ğŸ¯ ë‹¤ìŒ ë‹¨ê³„: ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ëª¨ë“ˆ êµ¬í˜„")


if __name__ == "__main__":
    main()